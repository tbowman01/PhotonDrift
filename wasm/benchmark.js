#!/usr/bin/env node
/**
 * Performance benchmarks for ADRScan WASM module
 * Compares WASM performance against theoretical native benchmarks
 */

const { ADRScan, utils } = require('./index.js');
const { performance } = require('perf_hooks');

// Generate test data
function generateTestFiles(count = 100) {
    const files = {};
    const technologies = ['mongodb', 'redis', 'docker', 'kubernetes', 'react', 'vue', 'angular', 'node', 'python', 'java'];
    
    for (let i = 0; i < count; i++) {
        const tech = technologies[i % technologies.length];
        const filename = `src/test-${i}.js`;
        files[filename] = `
// Test file ${i}
const ${tech} = require('${tech}');
const express = require('express');

class TestService {
    constructor() {
        this.${tech}Client = new ${tech}.Client();
    }
    
    async process() {
        return await this.${tech}Client.query();
    }
}

module.exports = TestService;
`.repeat(Math.floor(Math.random() * 5) + 1);
    }
    
    return files;
}

function generateTestAdr() {
    return `---
title: Test ADR for benchmarking
status: accepted
date: ${new Date().toISOString().split('T')[0]}
tags: [performance, testing]
---

# Test ADR for Benchmarking

This is a test ADR used for performance benchmarking.

## Status

Accepted

## Context

We need to measure the performance of ADR parsing and drift detection.

## Decision

We will use this test ADR to benchmark the WebAssembly module performance.

## Consequences

- Performance metrics will be available
- Comparisons can be made with native implementations
- Optimization opportunities can be identified
`;
}

async function runBenchmarks() {
    console.log('ðŸŽï¸  ADRScan WASM Performance Benchmarks\n');
    
    const scanner = new ADRScan({
        adrDir: './docs/adr',
        templateFormat: 'madr',
        driftEnabled: true
    });
    
    // Benchmark 1: Module initialization
    console.log('ðŸ“Š Benchmark 1: Module Initialization');
    const initStart = performance.now();
    const version = utils.version();
    const features = utils.features();
    const template = utils.getDefaultTemplate();
    const initEnd = performance.now();
    console.log(`   Time: ${(initEnd - initStart).toFixed(2)}ms`);
    console.log(`   Version: ${version}`);
    console.log(`   Features: ${features.length}`);
    console.log(`   Template size: ${template.length} chars\n`);
    
    // Benchmark 2: ADR Parsing
    console.log('ðŸ“Š Benchmark 2: ADR Parsing');
    const adrContent = generateTestAdr();
    const parseIterations = 1000;
    
    const parseStart = performance.now();
    for (let i = 0; i < parseIterations; i++) {
        scanner.parseAdr(adrContent, `test-${i}.md`);
    }
    const parseEnd = performance.now();
    
    const parseTime = parseEnd - parseStart;
    const parsePerOperation = parseTime / parseIterations;
    console.log(`   Total time: ${parseTime.toFixed(2)}ms`);
    console.log(`   Per operation: ${parsePerOperation.toFixed(3)}ms`);
    console.log(`   Operations/sec: ${(1000 / parsePerOperation).toFixed(0)}\n`);
    
    // Benchmark 3: Drift Detection (Small scale)
    console.log('ðŸ“Š Benchmark 3: Drift Detection (Small - 10 files)');
    const smallFiles = generateTestFiles(10);
    
    const driftSmallStart = performance.now();
    const driftSmallReport = scanner.detectDrift(smallFiles);
    const driftSmallEnd = performance.now();
    
    console.log(`   Time: ${(driftSmallEnd - driftSmallStart).toFixed(2)}ms`);
    console.log(`   Files processed: ${Object.keys(smallFiles).length}`);
    console.log(`   Drift items found: ${driftSmallReport.total_items}`);
    console.log(`   Time per file: ${((driftSmallEnd - driftSmallStart) / Object.keys(smallFiles).length).toFixed(3)}ms\n`);
    
    // Benchmark 4: Drift Detection (Medium scale)
    console.log('ðŸ“Š Benchmark 4: Drift Detection (Medium - 100 files)');
    const mediumFiles = generateTestFiles(100);
    
    const driftMediumStart = performance.now();
    const driftMediumReport = scanner.detectDrift(mediumFiles);
    const driftMediumEnd = performance.now();
    
    console.log(`   Time: ${(driftMediumEnd - driftMediumStart).toFixed(2)}ms`);
    console.log(`   Files processed: ${Object.keys(mediumFiles).length}`);
    console.log(`   Drift items found: ${driftMediumReport.total_items}`);
    console.log(`   Time per file: ${((driftMediumEnd - driftMediumStart) / Object.keys(mediumFiles).length).toFixed(3)}ms\n`);
    
    // Benchmark 5: Proposal Generation
    console.log('ðŸ“Š Benchmark 5: Proposal Generation');
    const proposeStart = performance.now();
    const proposals = await scanner.propose(driftMediumReport);
    const proposeEnd = performance.now();
    
    console.log(`   Time: ${(proposeEnd - proposeStart).toFixed(2)}ms`);
    console.log(`   Proposals generated: ${proposals.length}`);
    console.log(`   Time per proposal: ${((proposeEnd - proposeStart) / Math.max(1, proposals.length)).toFixed(3)}ms\n`);
    
    // Benchmark 6: Template Validation
    console.log('ðŸ“Š Benchmark 6: Template Validation');
    const templateValidations = 10000;
    const testTemplate = utils.getDefaultTemplate();
    
    const validateStart = performance.now();
    for (let i = 0; i < templateValidations; i++) {
        utils.validateTemplate(testTemplate);
    }
    const validateEnd = performance.now();
    
    const validateTime = validateEnd - validateStart;
    const validatePerOperation = validateTime / templateValidations;
    console.log(`   Total time: ${validateTime.toFixed(2)}ms`);
    console.log(`   Per validation: ${validatePerOperation.toFixed(4)}ms`);
    console.log(`   Validations/sec: ${(1000 / validatePerOperation).toFixed(0)}\n`);
    
    // Summary
    console.log('ðŸ“ˆ Performance Summary');
    console.log('====================');
    console.log(`   ADR Parsing: ${(1000 / parsePerOperation).toFixed(0)} ops/sec`);
    console.log(`   Drift Detection: ${((Object.keys(mediumFiles).length * 1000) / (driftMediumEnd - driftMediumStart)).toFixed(0)} files/sec`);
    console.log(`   Template Validation: ${(1000 / validatePerOperation).toFixed(0)} ops/sec`);
    console.log(`   Memory Usage: ${Math.round(process.memoryUsage().heapUsed / 1024 / 1024)} MB\n`);
    
    // Estimated vs Native comparison
    console.log('ðŸ”¬ Estimated WASM vs Native Performance');
    console.log('=====================================');
    console.log('   ADR Parsing: ~85% of native speed (estimated)');
    console.log('   Drift Detection: ~90% of native speed (estimated)');
    console.log('   Template Validation: ~95% of native speed (estimated)');
    console.log('   Startup Time: +50ms overhead for WASM initialization');
    console.log('   Memory: +2-5MB overhead for WASM runtime\n');
    
    console.log('âœ… Benchmarks completed successfully!');
    
    return {
        parseOpsPerSec: 1000 / parsePerOperation,
        driftFilesPerSec: (Object.keys(mediumFiles).length * 1000) / (driftMediumEnd - driftMediumStart),
        validateOpsPerSec: 1000 / validatePerOperation,
        memoryUsageMB: Math.round(process.memoryUsage().heapUsed / 1024 / 1024)
    };
}

// Run benchmarks if called directly
if (require.main === module) {
    runBenchmarks().then(results => {
        console.log('\nðŸ“Š Benchmark Results JSON:');
        console.log(JSON.stringify(results, null, 2));
        process.exit(0);
    }).catch(error => {
        console.error('ðŸ’¥ Benchmark failed:', error);
        process.exit(1);
    });
}

module.exports = { runBenchmarks };